{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: huangyongye <br/>\n",
    "@creat_date: 2017-05-17 <br/>\n",
    "在上个例子中，我们已经理解了在 TensorFlow 中如何来实现 LSTM， 在本例子中来实现以下 GRU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time \n",
    "\n",
    "# 设置 GPU 按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# 首先导入数据，看一下数据的形式\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "print mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 一、首先设置好模型用到的各个超参数 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "# 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式\n",
    "batch_size = tf.placeholder(tf.int32, [])  # 注意类型必须为 tf.int32\n",
    "# batch_size = 128\n",
    "\n",
    "# 每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素\n",
    "input_size = 28\n",
    "# 时序持续长度为28，即每做一次预测，需要先输入28行\n",
    "timestep_size = 28\n",
    "# 隐含层的数量\n",
    "hidden_size = 256\n",
    "# LSTM layer 的层数\n",
    "layer_num = 3\n",
    "# 最后输出分类类别数量，如果是回归预测的话应该是 1\n",
    "class_num = 10\n",
    "\n",
    "_X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, class_num])\n",
    "keep_prob = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 二、开始搭建 GRU 模型，和 LSTM 模型基本一致 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 把784个点的字符信息还原成 28 * 28 的图片\n",
    "# 下面几个步骤是实现 RNN / LSTM 的关键\n",
    "####################################################################\n",
    "# **步骤1：RNN 的输入shape = (batch_size, timestep_size, input_size) \n",
    "X = tf.reshape(_X, [-1, 28, 28])\n",
    "\n",
    "# 在 tf 1.0.0 版本中，可以使用上面的 三个步骤创建多层 lstm， 但是在 tf 1.1.0 版本中，可以通过下面方式来创建\n",
    "def gru_cell():\n",
    "    cell = rnn.GRUCell(hidden_size, reuse=tf.get_variable_scope().reuse)\n",
    "    return rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "mgru_cell = tf.contrib.rnn.MultiRNNCell([gru_cell() for _ in range(layer_num)], state_is_tuple = True)\n",
    "\n",
    "\n",
    "# **步骤5：用全零来初始化state\n",
    "init_state = mgru_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# **步骤6：方法一，调用 dynamic_rnn() 来让我们构建好的网络运行起来\n",
    "# ** 当 time_major==False 时， outputs.shape = [batch_size, timestep_size, hidden_size] \n",
    "# ** 所以，可以取 h_state = outputs[:, -1, :] 作为最后输出\n",
    "# ** state.shape = [layer_num, 2, batch_size, hidden_size], \n",
    "# ** 或者，可以取 h_state = state[-1]作为最后输出\n",
    "# ** 最后输出维度是 [batch_size, hidden_size]\n",
    "# outputs, state = tf.nn.dynamic_rnn(mgru_cell, inputs=X, initial_state=init_state, time_major=False)\n",
    "# h_state = state[-1]\n",
    "\n",
    "# *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************\n",
    "# 通过查看文档你会发现， RNNCell 都提供了一个 __call__()函数，我们可以用它来展开实现LSTM按时间步迭代。\n",
    "# **步骤6：方法二，按时间步展开计算\n",
    "outputs = list()\n",
    "state = init_state\n",
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        if timestep > 0:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        # 这里的state保存了每一层 LSTM 的状态\n",
    "        (cell_output, state) = mgru_cell(X[:, timestep, :],state)\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 三、最后设置 loss function 和 优化器，展开训练并完成测试 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter1, step 500, training accuracy 0.992188\n",
      "Iter2, step 1000, training accuracy 0.953125\n",
      "Iter3, step 1500, training accuracy 0.992188\n",
      "Iter4, step 2000, training accuracy 1\n",
      "Iter5, step 2500, training accuracy 0.992188\n",
      "Iter6, step 3000, training accuracy 0.984375\n",
      "Iter8, step 3500, training accuracy 0.992188\n",
      "Iter9, step 4000, training accuracy 1\n",
      "test accuracy 0.9892\n",
      "Time cost 237.486\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# 以下部分其实和之前写的多层 CNNs 来实现 MNIST 分类是一样的。\n",
    "# 只是在测试的时候也要设置一样的 batch_size.\n",
    "\n",
    "# 上面 GRU 部分的输出会是一个 [hidden_size] 的tensor，我们要分类的话，还需要接一个 softmax 层\n",
    "# 首先定义 softmax 的连接权重矩阵和偏置\n",
    "# out_W = tf.placeholder(tf.float32, [hidden_size, class_num], name='out_Weights')\n",
    "# out_bias = tf.placeholder(tf.float32, [class_num], name='out_bias')\n",
    "# 开始训练和测试\n",
    "W = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n",
    "y_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\n",
    "\n",
    "\n",
    "# 损失和评估函数\n",
    "cross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "time0 = time.time()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(4000):\n",
    "    _batch_size = 128\n",
    "    batch = mnist.train.next_batch(_batch_size)\n",
    "    if (i+1)%500 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            _X:batch[0], y: batch[1], keep_prob: 1.0, batch_size: _batch_size})\n",
    "        # 已经迭代完成的 epoch 数: mnist.train.epochs_completed\n",
    "        print \"Iter%d, step %d, training accuracy %g\" % ( mnist.train.epochs_completed, (i+1), train_accuracy)\n",
    "    sess.run(train_op, feed_dict={_X: batch[0], y: batch[1], keep_prob: 0.5, batch_size: _batch_size})\n",
    "\n",
    "# 计算测试数据的准确率\n",
    "print \"test accuracy %g\"% sess.run(accuracy, feed_dict={\n",
    "    _X: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0, batch_size:mnist.test.images.shape[0]})\n",
    "print 'Time cost %g' % (time.time() - time0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和前面 lstm 的例子比较一下，二者的准确率并没有太大的差别。在同样迭代 4000 次的条件下：\n",
    "- 运行时间为： lstm 248.446s　　　gru 244.955\n",
    "- 占用显存为： lstm 727m　　　　　gru 471m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、可视化看看 GRU 的是怎么做分类的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "毕竟 GRU更多的是用来做时序相关的问题，要么是文本，要么是序列预测之类的，所以很难像 CNNs 一样非常直观地看到每一层中特征的变化。在这里，我想通过可视化的方式来帮助大家理解 GRU 是怎么样一步一步地把图片正确的给分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 784) (5, 10)\n",
      "_outputs.shape = (28, 5, 256)\n",
      "arr_state.shape = (3, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "# 手写的结果 shape\n",
    "_batch_size = 5\n",
    "X_batch, y_batch = mnist.test.next_batch(_batch_size)\n",
    "print X_batch.shape, y_batch.shape\n",
    "_outputs, _state = np.array(sess.run([outputs, state],feed_dict={\n",
    "            _X: X_batch, y: y_batch, keep_prob: 1.0, batch_size: _batch_size}))\n",
    "print '_outputs.shape =', np.asarray(_outputs).shape\n",
    "print 'arr_state.shape =', np.asarray(_state).shape\n",
    "# 可见 outputs.shape = [ batch_size, timestep_size, hidden_size]\n",
    "# state.shape = [layer_num, 2, batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 784) (5, 10)\n",
      "_outputs.shape = (28, 5, 256)\n",
      "arr_state.shape = (3, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "_batch_size = 5\n",
    "X_batch, y_batch = mnist.test.next_batch(_batch_size)\n",
    "print X_batch.shape, y_batch.shape\n",
    "_outputs, _state = sess.run([outputs, state],feed_dict={\n",
    "            _X: X_batch, y: y_batch, keep_prob: 1.0, batch_size: _batch_size})\n",
    "print '_outputs.shape =', np.asarray(_outputs).shape\n",
    "print 'arr_state.shape =', np.asarray(_state).shape\n",
    "# 可见 outputs.shape = [ batch_size, timestep_size, hidden_size]\n",
    "# state.shape = [layer_num, 2, batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下面我找了一个字符 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.labels[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来看看这个字符样子,上半部分还挺像 2 来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9L0mG1AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41b4044150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X3 = mnist.train.images[27]\n",
    "img3 = X3.reshape([28, 28])\n",
    "plt.imshow(img3, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看在分类的时候，一行一行地输入，分为各个类别的概率会是什么样子的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 1, 256)\n",
      "(28, 256)\n"
     ]
    }
   ],
   "source": [
    "X3.shape = [-1, 784]\n",
    "y_batch = mnist.train.labels[0]\n",
    "y_batch.shape = [-1, class_num]\n",
    "\n",
    "X3_outputs = np.array(sess.run(outputs, feed_dict={\n",
    "            _X: X3, y: y_batch, keep_prob: 1.0, batch_size: 1}))\n",
    "print X3_outputs.shape\n",
    "X3_outputs.shape = [28, hidden_size]\n",
    "print X3_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABXFJREFUeJzt3UF24jgAQEGYk3BMLsb5PIv0bObRIQGEJf+qfUCRzZdi\nDDlv23YCoOOfvQcAwGcJP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAj/BC7X23a5\n3pb60qSVxrvSWE+n9ca7khVfayMsHf7vDuKoA/zsYzrhgFksHf7ZjAr7HgvGbAvVbOOBlU0f/lG7\n9u8etxKY2WI623jgqKYPP/zEK5fg3j2WFbmE2SL8vJ2/pmBuwg+DWOSYlfDDDiwKHTMea+GHgyjd\nwjxjTN9t5PwJP0QUYsnPCD/ADxxp4RR+WIjbJ/nPK+eB8APsZK+FXPiBQ/FX0WPCD/Ci1RYa4QeI\nEX6AU+sSkfADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QMx52xKfVwDgDzt+gBjhB4gRfoAY4QeI\nEX6AGOFfwGpfFbvaeFdS+urglax2TIQfIlaLE+MIP0CM8APECP8EXLcFPkn4AWKEHyBG+AFihP8O\n19uBI0uG35upQFky/ABlwg8QI/y/5BIRsLrpw/9saPe4jv/d83lf4TFzBJ8xffgBeC/hB4gRfoAY\n4QeIEX6AmKXD7y4QgN9bOvwA71LaSAo/h/fo8xWfHAvMQPgBYoQfIEb4w450TfMovwd8gvBz16NF\n4ZXvUBoxnhkV3luY8bh8N6YZx/s3I8d63rYl5gCAN7HjB4gRfoAY4QeIEX6AGOEHiBH+X1rlVrAV\nrXSr3YrM7Tirza3wA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPDDZFb7+D/rEX6e\nIk6wLuEHiBF+gBjhB4gRfoAY4Qcy/LOfL8LPXV4gcFzCDzuwqHI67bfBEn6AGOEHiFk6/N/9meQa\nNcB9S4f/WRaFscwte/L6fiwZfljVHlGrRLS0YEwf/j0OxLPP+d3PvXJZatQcrBSRV+Zoj7kd/dh/\ne74R5+6zzzny/DpKFx793Khz97xtiQUOgD+m3/ED8F7CDxAj/AAxwg8QI/wAMcL/IaV7hPdgbscx\nt+P4kjYAPkL4AWKEHyBG+AFihB8gRvgBYoT/l9zaBqxO+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFi\nhB8gRvgBYoQfIEb4AWKEHyBG+AFiDht+/9wc4L7Dhh/g/2wIvyTD7+DPyXGBz0iGfzViOI65HcdC\nPq/ztjkuACV2/AAxwg8QI/wAMcIPECP8ADHCDxBz2PC7h3gsczuOuR1HF74cNvwA3Cf8ADHCDxAj\n/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8\nADHCDxAj/B9yud62y/W27T0OAOEHiBF+gJjztrn6AFBixw8QI/wAMcIPECP8ADHCDxAj/AAxhw2/\nT8qOZW734bwepzS3hw0/APcJP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzw\nA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/b3e53ra9xwD83XnbvEYBSuz4\nAWKEHyBG+AFihB8gRvgBYoQfICYZ/sv1trnXfBxzO45zd5zS3CbDD1Am/AAxwg8QI/wAMcIPECP8\nADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wA\nMcIPECP8ADHnbdv2HgMAH2THDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8\nADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPEPMvf/XsLKhKxpkAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41aa6fb390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_W = sess.run(W, feed_dict={\n",
    "            _X:X3, y: y_batch, keep_prob: 1.0, batch_size: 1})\n",
    "h_bias = sess.run(bias, feed_dict={\n",
    "            _X:X3, y: y_batch, keep_prob: 1.0, batch_size: 1})\n",
    "h_bias.shape = [-1, 10]\n",
    "\n",
    "bar_index = range(class_num)\n",
    "for i in xrange(X3_outputs.shape[0]):\n",
    "    plt.subplot(7, 4, i+1)\n",
    "    X3_h_shate = X3_outputs[i, :].reshape([-1, hidden_size])\n",
    "    pro = sess.run(tf.nn.softmax(tf.matmul(X3_h_shate, h_W) + h_bias))\n",
    "    plt.bar(bar_index, pro[0], width=0.2 , align='center')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的图中，为了更清楚地看到线条的变化，我把坐标都去了，每一行显示了 4 个图，共有 7 行，表示了一行一行读取过程中，模型对字符的识别。可以看到，在只看到前面的几行像素时，模型根本认不出来是什么字符，随着看到的像素越来越多，最后就基本确定了它是字符 3."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
