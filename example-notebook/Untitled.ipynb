{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 不打印 warning \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs_2:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
      "conv1 Tensor(\"conv1/Relu:0\", shape=(?, 62, 62, 96), dtype=float32)\n",
      "pool1 Tensor(\"pool1/MaxPool:0\", shape=(?, 30, 30, 96), dtype=float32)\n",
      "conv2 <function conv2d at 0x7f50ac498950>\n",
      "pool2 Tensor(\"pool2/MaxPool:0\", shape=(?, 14, 14, 256), dtype=float32)\n",
      "conv3 Tensor(\"conv3/Relu:0\", shape=(?, 14, 14, 384), dtype=float32)\n",
      "conv4 Tensor(\"conv4/Relu:0\", shape=(?, 14, 14, 384), dtype=float32)\n",
      "conv5 Tensor(\"conv5/Relu:0\", shape=(?, 14, 14, 256), dtype=float32)\n",
      "pool5 Tensor(\"pool5/MaxPool:0\", shape=(?, 6, 6, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def conv2d(scope, x, filter_shape, strides_x, strides_y, padding, std=0.01, biases=0.1):\n",
    "    \"\"\" 高斯初始化： std=0.01, biases=0.1\n",
    "    Args:\n",
    "        scope: scope name of this layer.\n",
    "        x: 4-D inputs. [batch_size, in_height, in_width, in_channels]\n",
    "        filter_shape: A list of ints.[filter_height, filter_width, in_channels, out_channels]\n",
    "        strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input.\n",
    "        padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "    Returns:\n",
    "        h_conv:  A 4-D tensor. [batch_size, out_height, out_width, out_channels].\n",
    "        if padding is 'SAME', then out_height==in_height.\n",
    "        else, out_height = in_height - filter_height + 1.\n",
    "        the same for out_width.\n",
    "    \"\"\"\n",
    "    assert padding in ['SAME', 'VALID']\n",
    "    strides = [1, strides_x, strides_y, 1]\n",
    "    with tf.variable_scope(scope):\n",
    "        W_conv = tf.Variable(tf.truncated_normal(filter_shape, dtype=tf.float32, stddev=std), name='weights')\n",
    "        b_conv = tf.Variable(tf.constant(biases, shape=[filter_shape[-1]], dtype=tf.float32), name='biases')\n",
    "        h_conv = tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)\n",
    "        h_conv_relu = tf.nn.relu(h_conv + b_conv)\n",
    "    return h_conv_relu\n",
    "\n",
    "def max_pooling(scope, x, k_height, k_width, strides_x, strides_y, padding='SAME'):\n",
    "    \"\"\"max pooling layer.\"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        ksize = [1, k_height, k_width, 1]\n",
    "        strides = [1, strides_x, strides_y, 1]\n",
    "        h_pool = tf.nn.max_pool(x, ksize, strides, padding)\n",
    "    return h_pool\n",
    "\n",
    "\n",
    "\n",
    "inputs = tf.placeholder(dtype=tf.float32, shape=[None, 256, 256, 3], name='inputs')\n",
    "print(inputs)\n",
    "\n",
    "# 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "conv1 = conv2d('conv1', inputs, [11, 11, 3, 96], 4, 4, 'VALID', biases=0.0)\n",
    "pool1 = max_pooling('pool1', conv1, 3, 3, 2, 2, 'VALID')\n",
    "print('conv1', conv1)\n",
    "print('pool1', pool1)\n",
    "\n",
    "# 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
    "conv2 = conv2d('conv2', pool1, [5, 5, 96, 256], 1, 1, 'SAME')\n",
    "pool2 = max_pooling('pool2', conv2, 3, 3, 2, 2, 'VALID')\n",
    "print('conv2', conv2d)\n",
    "print('pool2', pool2)\n",
    "\n",
    "# 3rd Layer: Conv (w ReLu)\n",
    "conv3 = conv2d('conv3', pool2, [3, 3, 256, 384], 1, 1, 'SAME')\n",
    "print('conv3', conv3)\n",
    "\n",
    "# 4th Layer: Conv (w ReLu)\n",
    "conv4 = conv2d('conv4', conv3, [3, 3, 384, 384], 1, 1, 'SAME')\n",
    "print('conv4', conv4)\n",
    "\n",
    "# 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "conv5 = conv2d('conv5', conv4, [3, 3, 384, 256], 1, 1, 'SAME')\n",
    "pool5 = max_pooling('pool5', conv5, 3, 3, 2, 2, 'VALID')\n",
    "print('conv5', conv5)\n",
    "print('pool5', pool5)\n",
    "\n",
    "\n",
    "# # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "# flattened = tf.reshape(pool5, [-1, 6 * 6 * 256])\n",
    "# fc6 = fc('fc6', flattened, 6 * 6 * 256, 4096, std=0.005, biases=0.1, use_relu=True)\n",
    "# dropout6 = dropout(fc6, keep_prob)\n",
    "\n",
    "# # 7th Layer: FC (w ReLu) -> Dropout\n",
    "# fc7 = fc('fc7', dropout6, 4096, 4096, std=0.005, biases=0.1, use_relu=True)\n",
    "# dropout7 = dropout(fc7, keep_prob)\n",
    "\n",
    "# # 8th Layer: FC and return unscaled activations\n",
    "# fc8 = fc('fc8', dropout7, 4096, n_class, std=0.01, biases=0.0, use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 * 6 * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
