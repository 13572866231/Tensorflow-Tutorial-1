{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: huangyongye <br/>\n",
    "@creat_date: 2017-03-09 <br/>\n",
    "通过本例，你可以了解到单层 LSTM 的实现，多层 LSTM 的实现。输入输出数据的格式。 RNN 的 dropout layer 的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 设置 GPU 按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# 首先导入数据，看一下数据的形式\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "print mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 一、首先设置好模型用到的各个超参数 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "# 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式\n",
    "batch_size = tf.placeholder(tf.int32)  # 注意类型必须为 tf.int32\n",
    "# batch_size = 128\n",
    "\n",
    "# 每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素\n",
    "input_size = 28\n",
    "# 时序持续长度为28，即每做一次预测，需要先输入28行\n",
    "timestep_size = 28\n",
    "# 隐含层的数量\n",
    "hidden_size = 256\n",
    "# LSTM layer 的层数\n",
    "layer_num = 2\n",
    "# 最后输出分类类别数量，如果是回归预测的话应该是 1\n",
    "class_num = 10\n",
    "\n",
    "_X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, class_num])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 二、开始搭建 LSTM 模型，其实普通 RNNs 模型也一样 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 把784个点的字符信息还原成 28 * 28 的图片\n",
    "# 下面几个步骤是实现 RNN / LSTM 的关键\n",
    "####################################################################\n",
    "# **步骤1：RNN 的输入shape = (batch_size, timestep_size, input_size) \n",
    "X = tf.reshape(_X, [-1, 28, 28])\n",
    "\n",
    "# **步骤2：定义一层 LSTM_cell，只需要说明 hidden_size, 它会自动匹配输入的 X 的维度\n",
    "lstm_cell = rnn.BasicLSTMCell(num_units=hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "# **步骤3：添加 dropout layer, 一般只设置 output_keep_prob\n",
    "lstm_cell = rnn.DropoutWrapper(cell=lstm_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "\n",
    "# **步骤4：调用 MultiRNNCell 来实现多层 LSTM\n",
    "mlstm_cell = rnn.MultiRNNCell([lstm_cell] * layer_num, state_is_tuple=True)\n",
    "\n",
    "# **步骤5：用全零来初始化state\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# **步骤6：方法一，调用 dynamic_rnn() 来让我们构建好的网络运行起来,重点看 state = layer_num * [c_state, h_state],我们取 state[-1][1] 作为最后输出\n",
    "# outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False)\n",
    "# h_state = state[-1][1]\n",
    "\n",
    "# *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************\n",
    "# 通过查看文档你会发现， RNNCell 都提供了一个 __call__()函数，我们可以用它来展开实现LSTM按时间步迭代。\n",
    "# **步骤6：方法二，按时间步展开计算\n",
    "outputs = list()\n",
    "state = init_state\n",
    "h_state_list = list() # 非必要，只是为了后面可视化加上来而已\n",
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        if timestep > 0:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        # 这里的state保存了每一层 LSTM 的状态\n",
    "        (cell_output, state) = mlstm_cell(X[:, timestep, :], state)\n",
    "#         h_state_list.append(state[-1][1])    # state[-1][1] == vrll_output\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 三、最后设置 loss function 和 优化器，展开训练并完成测试 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter0, step 200, training accuracy 0.875\n",
      "Iter0, step 400, training accuracy 0.96875\n",
      "Iter1, step 600, training accuracy 0.976562\n",
      "Iter1, step 800, training accuracy 0.96875\n",
      "Iter2, step 1000, training accuracy 0.984375\n",
      "Iter2, step 1200, training accuracy 0.984375\n",
      "Iter3, step 1400, training accuracy 0.976562\n",
      "Iter3, step 1600, training accuracy 0.976562\n",
      "Iter4, step 1800, training accuracy 0.992188\n",
      "Iter4, step 2000, training accuracy 0.992188\n",
      "test accuracy 0.9825\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# 以下部分其实和之前写的多层 CNNs 来实现 MNIST 分类是一样的。\n",
    "# 只是在测试的时候也要设置一样的 batch_size.\n",
    "\n",
    "# 上面 LSTM 部分的输出会是一个 [hidden_size] 的tensor，我们要分类的话，还需要接一个 softmax 层\n",
    "# 首先定义 softmax 的连接权重矩阵和偏置\n",
    "# out_W = tf.placeholder(tf.float32, [hidden_size, class_num], name='out_Weights')\n",
    "# out_bias = tf.placeholder(tf.float32, [class_num], name='out_bias')\n",
    "# 开始训练和测试\n",
    "W = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n",
    "y_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\n",
    "\n",
    "\n",
    "# 损失和评估函数\n",
    "cross_entropy = -tf.reduce_mean(y * tf.log(y_pre))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(2000):\n",
    "    _batch_size = 128\n",
    "    batch = mnist.train.next_batch(_batch_size)\n",
    "    if (i+1)%200 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={\n",
    "            _X:batch[0], y: batch[1], keep_prob: 1.0, batch_size: _batch_size})\n",
    "        # 已经迭代完成的 epoch 数: mnist.train.epochs_completed\n",
    "        print \"Iter%d, step %d, training accuracy %g\" % ( mnist.train.epochs_completed, (i+1), train_accuracy)\n",
    "    sess.run(train_op, feed_dict={_X: batch[0], y: batch[1], keep_prob: 0.5, batch_size: _batch_size})\n",
    "\n",
    "# 计算测试数据的准确率\n",
    "print \"test accuracy %g\"% sess.run(accuracy, feed_dict={\n",
    "    _X: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0, batch_size:mnist.test.images.shape[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一共只迭代不到5个epoch，在测试集上就已经达到了0.9825的准确率，可以看出来 LSTM 在做这个字符分类的任务上还是比较有效的，而且我们最后一次性对 10000 张测试图片进行预测，才占了 725 MiB 的显存。而我们在之前的两层 CNNs 网络中，预测 10000 张图片一共用了 8721 MiB 的显存，差了整整 12 倍呀！！ 这主要是因为 RNN/LSTM 网络中，每个时间步所用的权值矩阵都是共享的，可以通过前面介绍的 LSTM 的网络结构分析一下，整个网络的参数非常少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、可视化看看 LSTM 的是怎么做分类的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "毕竟 LSTM 更多的是用来做时序相关的问题，要么是文本，要么是序列预测之类的，所以很难像 CNNs 一样非常直观地看到每一层中特征的变化。在这里，我想通过可视化的方式来帮助大家理解 LSTM 是怎么样一步一步地把图片正确的给分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下面我找了一个字符 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来看看这个字符样子,上半部分还挺像 2 来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIU/VYkXdwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f58e9ac7450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X3 = mnist.train.images[1]\n",
    "img3 = X3.reshape([28, 28])\n",
    "plt.imshow(img3, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看在分类的时候，一行一行地输入，分为各个类别的概率会是什么样子的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X3_outputs.shape= (28, 1, 256)\n"
     ]
    }
   ],
   "source": [
    "X3.shape = [-1, 784]\n",
    "y_batch = mnist.train.labels[0]\n",
    "y_batch.shape = [-1, class_num]\n",
    "\n",
    "X3_outputs = np.array(sess.run(outputs,feed_dict={\n",
    "            _X: X3, y: y_batch, keep_prob: 1.0, batch_size: 1}))\n",
    "print 'X3_outputs.shape=', X3_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 784) (2, 10)\n",
      "X_outputs.shape= (28, 2, 256)\n"
     ]
    }
   ],
   "source": [
    "X_batch, y_batch = mnist.test.next_batch(2)\n",
    "print X_batch.shape, y_batch.shape\n",
    "X_outputs = np.array(sess.run(outputs,feed_dict={\n",
    "            _X: X_batch, y: y_batch, keep_prob: 1.0, batch_size: 2}))\n",
    "print 'X_outputs.shape=', X_outputs.shape\n",
    "# 可见 outputs.shape = [timestep_size, batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看这两部分其实是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 256)\n"
     ]
    }
   ],
   "source": [
    "X3_h_states.shape = [28, hidden_size]\n",
    "print X3_h_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 256)\n"
     ]
    }
   ],
   "source": [
    "X3_outputs.shape = [28, hidden_size]\n",
    "print X3_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngBjhB4j5ByAFao/4BUGPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe950145290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_W = sess.run(W, feed_dict={\n",
    "            _X:X3, y: y_batch, keep_prob: 1.0, batch_size: 1})\n",
    "h_bias = sess.run(bias, feed_dict={\n",
    "            _X:X3, y: y_batch, keep_prob: 1.0, batch_size: 1})\n",
    "h_bias.shape = [-1, 10]\n",
    "\n",
    "bar_index = range(class_num)\n",
    "for i in xrange(X3_outputs.shape[0]):\n",
    "    plt.subplot(7, 4, i+1)\n",
    "    X3_h_shate = X3_outputs[i, :].reshape([-1, hidden_size])\n",
    "    pro = sess.run(tf.nn.softmax(tf.matmul(X3_h_shate, h_W) + h_bias))\n",
    "    plt.bar(bar_index, pro[0], width=0.2 , align='center')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的图中，为了更清楚地看到线条的变化，我把坐标都去了，每一行显示了 4 个图，共有 7 行，表示了一行一行读取过程中，模型对字符的识别。可以看到，在只看到前面的几行像素时，模型根本认不出来是什么字符，随着看到的像素越来越多，最后就基本确定了它是字符 3."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
