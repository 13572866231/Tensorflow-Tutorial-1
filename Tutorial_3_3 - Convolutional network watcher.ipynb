{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CNN 网络结构查看\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 设置按需使用GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义卷积层\n",
    "def conv2d(name, x, filter_shape, strides_x, strides_y, padding):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        x: 4-D inputs. [batch_size, in_height, in_width, in_channels]\n",
    "        filter_shape: A list of ints.[filter_height, filter_width, in_channels, out_channels]\n",
    "        strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input.\n",
    "        padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "    Returns:\n",
    "        h_conv:  A 4-D tensor. [batch_size, out_height, out_width, out_channels]. \n",
    "        if padding is 'SAME', then out_height==in_height. \n",
    "        else, out_height = in_height - filter_height + 1.\n",
    "        the same for out_width.\n",
    "    \"\"\"\n",
    "    assert padding in ['SAME', 'VALID']\n",
    "    strides=[1,strides_x, strides_y,1]\n",
    "    with tf.variable_scope(name):\n",
    "        W_conv = tf.get_variable('w', shape=filter_shape)\n",
    "        b_conv = tf.get_variable('b', shape=[filter_shape[-1]])\n",
    "        h_conv = tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)\n",
    "        h_conv_relu = tf.nn.relu(h_conv + b_conv)\n",
    "    return h_conv_relu\n",
    "    \n",
    "\n",
    "def max_pooling(x, k_height, k_width, strides_x, strides_y, padding='SAME'):\n",
    "    \"\"\"max pooling layer.\"\"\"\n",
    "    ksize=[1,k_height, k_width,1]\n",
    "    strides=[1,strides_x, strides_y,1]\n",
    "    h_pool = tf.nn.max_pool(x, ksize, strides, padding)\n",
    "    return h_pool\n",
    "\n",
    "def dropout(x, keep_prob, name=None):\n",
    "    \"\"\"dropout layer\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob, name=name)\n",
    "\n",
    "def fc(name, x, in_size, out_size, activation=None):\n",
    "    \"\"\"fully-connect\n",
    "    Args:\n",
    "        x: 2-D tensor, [batch_size, in_size]\n",
    "        in_size: the size of input tensor.\n",
    "        out_size: the size of output tensor.\n",
    "        activation: 'relu' or 'sigmoid' or 'tanh'.\n",
    "    Returns:\n",
    "        h_fc: 2-D tensor, [batch_size, out_size].\n",
    "    \"\"\"\n",
    "    if activation is not None:\n",
    "        assert activation in ['relu', 'sigmoid', 'tanh'], 'Wrong activation function.'\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', shape = [in_size, out_size], dtype=tf.float32)\n",
    "        b = tf.get_variable('b', [out_size], dtype=tf.float32)\n",
    "        h_fc = tf.nn.xw_plus_b(x, w, b)\n",
    "        if activation == 'relu':\n",
    "            return tf.nn.relu(h_fc)\n",
    "        elif activation == 'tanh':\n",
    "            return tf.nn.tanh(h_fc)\n",
    "        elif activation == 'sigmoid':\n",
    "            return tf.nn.sigmoid(h_fc)\n",
    "        else:\n",
    "            return h_fc\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    X_ = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    X = tf.reshape(X_, [-1, 224, 224, 3])\n",
    "\n",
    "# # 把X转为卷积所需要的形式\n",
    "# with tf.variable_scope('cnn_inference'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 自定义的网络\n",
    "# conv1 = conv2d('conv1', X, [15, 15, 3, 64], 3, 3, 'VALID')\n",
    "# print(conv1)\n",
    "# pool1 = max_pooling(conv1, 3, 3, 2, 2, 'VALID')\n",
    "# print(pool1)\n",
    "\n",
    "# conv2 = conv2d('conv2', pool1, [5, 5, 64, 128], 1, 1, 'VALID')\n",
    "# print(conv2)\n",
    "# pool2 = max_pooling(conv2, 3, 3, 2, 2, 'VALID')\n",
    "# print(pool2)\n",
    "\n",
    "# conv3 = conv2d('conv3', pool2, [3, 3, 128, 256], 1, 1, 'SAME')\n",
    "# print(conv3)\n",
    "# conv4 = conv2d('conv4', conv3, [3, 3, 256, 256], 1, 1, 'SAME')\n",
    "# print(conv4)\n",
    "# conv5 = conv2d('conv5', conv4, [3, 3, 256, 256], 1, 1, 'SAME')\n",
    "# print(conv5)\n",
    "# pool5 = max_pooling(conv5, 3, 3, 2, 2, 'VALID')\n",
    "# print(pool5)\n",
    "\n",
    "# conv6 = conv2d('conv6', pool5, [6, 6, 256, 512], 1, 1, 'VALID')\n",
    "# print(conv6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv1/Relu:0\", shape=(?, 54, 54, 64), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 26, 26, 64), dtype=float32)\n",
      "Tensor(\"conv2/Relu:0\", shape=(?, 26, 26, 192), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 12, 12, 192), dtype=float32)\n",
      "Tensor(\"conv3/Relu:0\", shape=(?, 12, 12, 384), dtype=float32)\n",
      "Tensor(\"conv4/Relu:0\", shape=(?, 12, 12, 384), dtype=float32)\n",
      "Tensor(\"conv5/Relu:0\", shape=(?, 12, 12, 256), dtype=float32)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 5, 5, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# alexnet v2\n",
    "conv1 = conv2d('conv1', X, [11, 11, 3, 64], 4, 4, 'VALID')\n",
    "print(conv1)\n",
    "pool1 = max_pooling(conv1, 3, 3, 2, 2, 'VALID')\n",
    "print(pool1)\n",
    "\n",
    "conv2 = conv2d('conv2', pool1, [5, 5, 64, 192], 1, 1, 'SAME')\n",
    "print(conv2)\n",
    "pool2 = max_pooling(conv2, 3, 3, 2, 2, 'VALID')\n",
    "print(pool2)\n",
    "\n",
    "conv3 = conv2d('conv3', pool2, [3, 3, 192, 384], 1, 1, 'SAME')\n",
    "print(conv3)\n",
    "conv4 = conv2d('conv4', conv3, [3, 3, 384, 384], 1, 1, 'SAME')\n",
    "print(conv4)\n",
    "conv5 = conv2d('conv5', conv4, [3, 3, 384, 256], 1, 1, 'SAME')\n",
    "print(conv5)\n",
    "pool5 = max_pooling(conv5, 3, 3, 2, 2, 'VALID')\n",
    "print(pool5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fc6/Relu:0\", shape=(?, 1, 1, 4096), dtype=float32)\n",
      "Tensor(\"dropout/mul:0\", shape=(?, 1, 1, 4096), dtype=float32)\n",
      "Tensor(\"fc7/Relu:0\", shape=(?, 1, 1, 4096), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "fc6 = conv2d('fc6', pool5, [5, 5, 256, 4096], 1, 1, 'VALID')\n",
    "print(fc6)\n",
    "drop6 = dropout(fc6, 0.5)\n",
    "print(drop6)\n",
    "fc7 = conv2d('fc7', drop6, [1, 1, 4096, 4096], 1, 1, 'SAME')\n",
    "print(fc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
